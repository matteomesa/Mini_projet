{
 "cells": [
  {
 {
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hot-Wheels - Thymio Path Finder Project (Mobile Robotics) \n",
    "##### _The best vision based path findind program for Thymio_\n",
    "***\n",
    "\n",
    "## Abstract\n",
    "<div style=\"text-align: justify\"> This project is part of the course Basics of mobile robotics (ME-452). The goal is to create a software enabling the Thymio to find its path through a set of obstacles. To this end, it will need to localize itself as well as its environment, and find the best path to the goal to then drive to it. Another challenge is for it to avoid any unexpected object it meets. This project is thus split in four distinct modules, plus the main file. Those four modules are: </div>\n",
    "\n",
    "- <b>Vision</b>, camera based function and transformation for position, goal and terrain exctraction\n",
    "- <b>Kalman Filter</b>, sensor fusion for accurate odometry computation\n",
    "- <b>Optimal path algorithm</b>, path optimization using Dijkstra\n",
    "- <b>Motion control</b>, in charge of the Thymio's motion and local avoidance\n",
    "\n",
    "Those modules are further detailed bellow:\n",
    "\n",
    "## Table of contents\n",
    "1. [Vision](#1-vision)\n",
    "2. [Kalman Filter](#2-kalman-filter)\n",
    "    - [Position Kalman Filter](#21-position-kalman-filter)\n",
    "    - [Orientation kalman filter](#22-orientation-kalman-filter)\n",
    "    - [Kalman filter update squence](#23-kalman-filter-update-sequence)\n",
    "3. [Optimal path algorithm](#3-optimal-path-algorithm)\n",
    "4. [Motion control](#4-motion-control)\n",
    "    - [Thymio's bivalent behavior](#41-thymios-bivalent-behavior)\n",
    "    - [Global path following](#42-global-path-following)\n",
    "    - [Local avoidance](#local-avoidance)"
     "5. [Demo](#5-demo)\n",
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vision\n",
    "\n",
    "<div style=\"text-align: justify\"> This module includes functions converting the information percieved by the camera into a map, a goal and the current position of the robot. To this end, the image obtained from the camera is first filtered using a median filter, useful to remove the noise while preserving clear edges.<br>\n",
    "\n",
    "The obstacles are represented by black shapes. In order to fetch the map, function <b>terrainFetch</b> computing the visibility graph is called. Pixel segmentation is done on the retrieved image, followed by a blob analysis only conserving blobs of a certain size (filtering leftover noise). These blobs are then dilated to account for the robot's size, and the corners for each dilated blob are computed. The start and end point (i.e. start position of the robot and goal position) are also fetched (using functions bellow), and added to the previous nodes (i.e. corners of the dilated blobs). To check weither the nodes are connected, each node is iterating over all the node, checking wether it has a direct connection (without intersecting a dilated blobs). This function returns the position of the nodes in meters, their connections, and the dilated obstacles' mask (displayed in the visual interface).<br>\n",
    "\n",
    "Function <b>poseFetch</b> fetches the robots' position and angle thanks to the Thymio's red (angle) and green (position & angle) LEDs. The position is computed by first running a pixel segmentation and blob analysis to only keep the two green blobs. The center of those two blobs is found and the middle between those two points is defined to be the position of the robot. The center of the red blob is obtained with the same transformations (this time for one blob only), and the angle of the robot is resolved when comparing the two center found (center of the red blob, and center of the two green blobs) with respect to the X axis (i.e. using the arctan). The position of the robot is returned in pixel since this function is used in <b>terrainFetch</b>, but the position in meter is returned by the function <b>fetchOdoMeters</b>, calling <b>odoFetch</b> multiple times and converting the result in meter.<br>\n",
    "\n",
    "The goal represented by a red square piece of paper is fetched with function <b>goalFetch</b>. It is retreived by performing the same transformation as done for the position : pixel segmentation followed by blob analysis to check the size of the blob and avoid noise. The goal is thus the center of the blob and is returned in meters.<br>\n",
    "\n",
    "In order to get a visual feedback on what the algorithm outputs, function <b>liveFeedback</b> was created, it prints each node and their connections, the dilated obstacles, the robot's current position and angle, and the goal position.</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TymioCam Feedback](../../../Desktop/ThymioRun.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Kalman Filter\n",
    "\n",
    "The state of the Thymio is estimated using a Kalman filter. The choice for a kalman filter was made because we decided for the global navigation to use nodes described in a coordinate system with x and y coordinates. Thus, also the position of the Thymio should be estimated in that coordinate system. Since we have multiple sensor readings: vision and wheel encoder, the Kalman filter was selected to fuse these measurements.\n",
    "\n",
    "Since the position dynamics are independent of the orientation dynamics, two Kalman filters are used. Both are initialized using a vision measurement.\n",
    "\n",
    "### 2.1 Position Kalman Filter\n",
    "The dynamic system used for the position estimation:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{k+1} \\\\ y_{k+1} \\\\ v_{x_{k+1}} \\\\ v_{y_{k+1}}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & T & 0 \\\\\n",
    "0 & 1 & 0 & T \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{k} \\\\ y_{k} \\\\ v_{x_{k}} \\\\ v_{y_{k}}\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "\\frac{T^2}{2} & 0 \\\\\n",
    "0 & \\frac{T^2}{2} \\\\\n",
    "T & 0 \\\\\n",
    "0 & T \\\\\n",
    "\\end{bmatrix}\\boldsymbol{w}_k\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_{1_k} = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0\\\\\n",
    "0 & 1 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{k} \\\\ y_{k} \\\\ v_{x_{k}} \\\\ v_{y_{k}}\n",
    "\\end{bmatrix} + \\boldsymbol{v}_{k_1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_{2_k} = \\begin{bmatrix}\n",
    "0 & 0 & 1 & 0\\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{k} \\\\ y_{k} \\\\ v_{x_{k}} \\\\ v_{y_{k}}\n",
    "\\end{bmatrix} + \\boldsymbol{v}_{k_2}\n",
    "$$\n",
    "\n",
    "The $x$ and $y$ position coordinates are in meters and the $v_x$ and $v_y$ velocities are in m/s. The output measurement $y_{1_k}$ consists of the position and represents the vision measurements. Output measurement $y_{2_k}$ consists of the velocity and represents the wheel encoder measurements from which the Thymio's linear velocity is determined. Following covariance matrices have been defines:\n",
    "\n",
    "$$\n",
    "W = \n",
    "\\begin{bmatrix}\n",
    "\\frac{T^2}{2} & 0 \\\\\n",
    "0 & \\frac{T^2}{2} \\\\\n",
    "T & 0 \\\\\n",
    "0 & T \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\sigma_{acc}^2 & 0 \\\\ \n",
    "0 & \\sigma_{acc}^2 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{T^2}{2} & 0 & T & 0 \\\\\n",
    "0 & \\frac{T^2}{2} & 0 & T\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "V_1 = \n",
    "\\begin{bmatrix}\n",
    "0.0001 & 0 \\\\\n",
    "0 & 0.0001 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "V_2 = \n",
    "\\begin{bmatrix}\n",
    "0.01 & 0 \\\\\n",
    "0 & 0.01 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The variance matrix $W$ corresponds to the process noise term $\\boldsymbol{w}_k$ and the variance matrices $V_1$ and $V_2$ correspond to the output noise terms $\\boldsymbol{v}_{k_1}$ and $\\boldsymbol{v}_{k_2}$. The parameter $\\sigma_{acc}$ represents the exptected standard deviation in linear acceleration. Since a constant velocity model is assumed, it accounts for model mismatch. $T$ is the time passed since the last estimate has been made.\n",
    "\n",
    "The output covariance matrix $V_1$ was chosen to have 0.0001 on its diagonal since this corresponds to a standard deviation of 0.01m or an uncertainty of 1cm in the position measurement. The output covariance matrix $V_2$ was chose to have 0.01 on its diagonal since this corresponds to a standard deviation of 0.1m/s or an uncertainty of 10cm/s in the angular velocity measurement.\n",
    "\n",
    "\n",
    "### 2.2 Orientation Kalman Filter\n",
    "The dynamic system used for the orientation estimation:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\theta_{k+1} \\\\ \\omega_{k+1}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "1 & T \\\\\n",
    "0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\theta_{k} \\\\ \\omega_{k}\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "\\frac{T^2}{2} \\\\\n",
    "T\n",
    "\\end{bmatrix}\\boldsymbol{w}_k\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_{1_k} = \\begin{bmatrix}\n",
    "1 & 0 \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\theta_{k} \\\\ \\omega_{k}\n",
    "\\end{bmatrix} + \\boldsymbol{v}_{k_1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_{2_k} = \\begin{bmatrix}\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\theta_{k} \\\\ \\omega_{k}\n",
    "\\end{bmatrix} + \\boldsymbol{v}_{k_2}\n",
    "$$\n",
    "\n",
    "The orientation angle $\\theta$ is in radians and the angular velocity $\\omega$ is in rad/s. The output measurement $y_{1_k}$ consists of the orientation and represents the vision measurements. Output measurement $y_{2_k}$ consists of the angular velocity and represents the wheel encoder measurements from which the Thymio's angular velocity is determined. Following covariance matrices have been defines:\n",
    "$$\n",
    "W = \n",
    "\\begin{bmatrix}\n",
    "\\frac{T^2}{2} \\\\\n",
    " T \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\sigma_{acc}^2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{T^2}{2} & T\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "V_1 = \n",
    "\\begin{bmatrix}\n",
    "0.0001\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "V_2 = \n",
    "\\begin{bmatrix}\n",
    "0.01\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "The variance matrix $W$ corresponds to the process noise term $\\boldsymbol{w}_k$ and the variance matrices $V_1$ and $V_2$ correspond to the output noise terms $\\boldsymbol{v}_{k_1}$ and $\\boldsymbol{v}_{k_2}$. The parameter $\\sigma_{acc}$ represents the exptected standard deviation in angular acceleration. Since a constant angular velocity model is assumed, it accounts for model mismatch. $T$ is the time passed since the last estimate has been made.\n",
    "\n",
    "The output covariance matrix $V_1$ was chosen to have 0.0001 since this corresponds to a standard deviation of 0.01rad or an uncertainty of 0.57deg in the orientation measurement. The output covariance matrix $V_2$ was chose to have 0.01 since this corresponds to a standard deviation of 0.1rad/s or an uncertainty of 5.7deg/s in the velocity measurement.\n",
    "\n",
    "### 2.3 Kalman filter update sequence\n",
    "The estimate of Thymio's state is performed through the following sequence of calculations:\n",
    "1. prediction step\n",
    "- $\\hat{x}_{k}^- = A\\hat{x}_{k-1}^+$\n",
    "- $P_k^- = AP_{k-1}^+A^T + W$\n",
    "\n",
    "2. update step vision measurement (if available)\n",
    "- $K_k = P_k^-C_{1}^T(C_1P_k^-C_1^T + V_1)^{-1}$\n",
    "- $\\hat{x}_k^+ = \\hat{x}_k^- + K_k(y_{1_k}-C_{1}\\hat{x}_k^-)$\n",
    "- $P_k^+ = (I-K_kC_1)P_k^-$\n",
    "\n",
    "3. update step wheel encoder (if available)\n",
    "- $K_k = P_k^-C_{2}^T(C_1P_k^-C_2^T + V_2)^{-1}$\n",
    "- $\\hat{x}_k^+ = \\hat{x}_k^- + K_k(y_{2_k}-C_{2}\\hat{x}_k^-)$\n",
    "- $P_k^+ = (I-K_kC_2)P_k^-$\n",
    "\n",
    "There three step are executed at each timestep and are the same for the position as well as the orientation kalman filter."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimal Path Algorithm\n",
    "<br/>\n",
    "<div style=\"text-align: justify\">\n",
    "To find the optimal path for the robot, we decide to implement Dijkstra’s algorithm on our project. The vision module extract all coordinates of nodes and edges from obstacles.  This data is sent thanks to the function <b>terrainFetch</b>. We run this function until detect the map correctly. This step is done on the function <b>opt_path </b>. When it’s done, we run the Dijkstra algorithm with our function <b>dijkstra </b>. We have created node and edges classes in order to have a better readability.\n",
    "<br/><br/>\n",
    "In this function, we initialize a list of nodes and another one with edges. Note that we have to duplicate edges to have both ways, because the vision module gives us only one way. Then, we define two lists to save the length of the path, and its coordinates of each node and one dictionary, which allows us to know the index of each node on its lists. \n",
    "<br/> <br/>\n",
    "We initialise three variables to define the current situation (current node, distance between it and the starting point, all nodes of its path).  The <b>iteration </b> variable helps us to define the future node after each iteration.\n",
    "<br/> <br/>\n",
    "As long as the edges list isn’t empty, we calculate the distance between the start and end point of each edge having as starting point the current node. Then, we update the two array <b>tabLenpath </b> and <b>tabPath </b>. I remove all used edges and ones which, the end point is the current node. At the end, we updated the situation increasing iteration by one and updating the three variables. We have limited the iterations by putting a condition in the while loop, to avoid having an infinite loop, in case the goal is isolated.\n",
    "<br/> <br/>\n",
    "When edge list is empty, we extract the optimal path by tabPath and index of goal position. In order for the motion control module to be able to use the output correctly we transform the list into an array.\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion Control\n",
    "\n",
    "### Thymio's bivalent behavior\n",
    "<div style=\"text-align: justify\">The motion control part of the program takes care of both the global path following and the local object avoidance. Follows the finite state machine for the controller to go from the global to the local behavior. Each time the controller quits the local loop, a new optimal path is recomputed to avoid collisions with walls on the map.\n",
    "\n",
    "![FSM Global to local](./img/FSM-GlobalToLocal.png \"Global to local behaviour FSM\")\n",
    "\n",
    "This FSM is implemented in the function `update_motion()` in the file [**motion_control.py**](motion_control.py) and is in charge of choosing which behavior to adopt to compute the update the wheels speed. Three functions are called :\n",
    " - `update_global()` to compute the wheels speed to follow the global optiml path\n",
    " - `update_local_pivot()` to pivot left when an obstacle is detected\n",
    " - `update_local_fwd()` to move forward 15cm once the obstacle is not in the path anymore\n",
    "\n",
    "### Global path following\n",
    "Because Thymio is a two-wheeled robot it can go strait forward by setting the right and left wheelspeed to the same value, and it can pivot by setting the wheel speed to opposite values. For simplicity and robustness to path-following, these are the two basic control movements we used to control our Thymio's global motion.\n",
    "\n",
    "We have decided to use a proportional controller for both the forward and pivot motion: the further the robot's state from the desired one, the faster the movement.</div>\n",
    "| Motion | Inputs | Computes | Outputs |\n",
    "|---:|---|---|---|\n",
    "| **Forward** | - Position of the robot, target point | - Distance to the target point | Wheel speed = $K \\cdot d(robot,target) + C$|\n",
    "| **Pivot** | - Angle of the robot, target point | - Difference in angle between the robot and the target point | Wheel speed = $\\pm K \\cdot \\alpha(robot,target) + C$|\n",
    "\n",
    "\n",
    "> It is important to note that the right and left wheel speeds differ a bit according the the angle between Thymio and the target point during the forward motion in order to track better the target. For example if the angle of Thymio is a bit off by $-\\pi/8$ then the right wheel will be a bit faster than the left wheel.\n",
    "\n",
    "> The proportional terms K and the offset values C were set after some tries on hardware in order to track the target point at best without having the robot going too fast.\n",
    "\n",
    "<div style=\"text-align: justify\">\n",
    "The below diagram explains what is the logic behind the optimal path following controller.\n",
    "\n",
    "![Global path following](./img/GlobalPathFollowing.png \"Optimal path following\")\n",
    "\n",
    "### Local avoidance\n",
    "In case there is an non intended object on the pre-computed optimal path, Thymio cannot pass through and has to find a way to get around it.\n",
    "\n",
    "We've tried first to implement a neuronal network controller as in class, each sensor having a weight on the robot's wheels speed. However Thymio only has front sensors and couldn't detect if an object is on its side, which often resulted in the robot to blindly charge into the obstacle:\n",
    "\n",
    "![Neuronal network problem](./img/NeuronalNetwork.png \"Blindly charge the obstacle\")\n",
    "\n",
    "The other solution was to pivot the robot 90° left once he detect something on the sensors, then do a right arc until either the obstacle is detected again or if the robot faces the target point again.\n",
    "\n",
    "This solution wasn't successful as the robot's arc radius is highly dependant on the object's size and the robot often ended charging the obstacle as in the previous method.\n",
    "\n",
    "Finally we decided to implement this final method. It's simple to implement and more robust than anything we tried before.\n",
    "\n",
    "```\n",
    "while sensors:\n",
    "    pivot left\n",
    "while no sensors:\n",
    "    go forward for 15 cm\n",
    "recompute optimal path\n",
    "```\n",
    "\n",
    "</div>"
    "<div style=\"text-align: justify\">\n",
    "This last part shows a simulation with three different views: the first one is an external camera shot, the second one is the real time feedback seen by the camera with the implementation of the vision and the optimal path and the last one is a plot used to observe the approximation of the robot position and its angle using the Kalmann filter\n",
    "<div>\n",
    "\n",
    "\n",
    "|                                    |                    |\n",
    "| ---------------------------------- | ------------------ |\n",
    "| Description                        |                    |\n",
    "| Simulation                         | ![](camera.gif)    |\n",
    "| Image Processing from Webcam       | ![](feedback.gif)  |\n",
    "| Kalman filter display              | ![](pygame.gif)    |\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE\n",
    "### Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% Thymio notebook\n"
    }
   },
   "outputs": [],
   "source": [
    "import tdmclient.notebook\n",
    "await tdmclient.notebook.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_python\n",
    "nf_leds_prox_h(0,0,0,0,0,0,0,0)\n",
    "nf_leds_prox_v(0,0)\n",
    "nf_leds_rc(0)\n",
    "nf_leds_sound(0)\n",
    "nf_leds_temperature(0,0)\n",
    "nf_leds_top(0,0,0)\n",
    "nf_leds_bottom_left(0,0,0)\n",
    "nf_leds_bottom_right(0,0,0)\n",
    "nf_leds_buttons(30,0,0,0)\n",
    "nf_leds_top(0,255,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.10.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "import cv2\n",
    "import pygame as pg\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "\n",
    "import motion_control\n",
    "import vision\n",
    "import kalman\n",
    "import AlgoGlobNav as gn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'AlgoGlobNav' from 'c:\\\\Users\\\\jacqu\\\\Downloads\\\\HW final github Zip\\\\Hot-Wheels-main\\\\AlgoGlobNav.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(motion_control)\n",
    "reload(vision)\n",
    "reload(kalman)\n",
    "reload(gn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%% Define function to control wheel speeds\n"
    }
   },
   "outputs": [],
   "source": [
    "@tdmclient.notebook.sync_var\n",
    "def motors(l_speed=500, r_speed=500):\n",
    "    \"\"\"\n",
    "    Sets the motor speeds of the Thymio\n",
    "    param l_speed: left motor speed\n",
    "    param r_speed: right motor speed\n",
    "    \"\"\"\n",
    "    global motor_left_target, motor_right_target\n",
    "\n",
    "    motor_left_target = l_speed\n",
    "    motor_right_target = r_speed\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "def read_motors():\n",
    "    \"\"\"\n",
    "    Read the motor speeds of the Thymio\n",
    "    \"\"\"\n",
    "    global motor_left_speed, motor_right_speed\n",
    "\n",
    "    return [motor_left_speed, motor_right_speed]\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "def read_proximity():\n",
    "    \"\"\"\n",
    "    Read the proximity sensors of the Thymio\n",
    "    \"\"\"\n",
    "    global prox_horizontal\n",
    "\n",
    "    return prox_horizontal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation Kalman Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%% Run thymio\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Kalman filter initialization\n",
      "Initialization complete: 0.788 s\n",
      "Initial pose: [0.07709167 0.71835417 0.         0.        ]\n",
      "Initial orientation [90.  0.]\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Kalman filter\n",
    "t = time.time()\n",
    "print(\"Start Kalman filter initialization\")\n",
    "mus_pos = []\n",
    "mus_att = []\n",
    "\n",
    "# Initialize state estimate using vision\n",
    "try:\n",
    "    pos, orientation = vision.fetchPoseMeters(vid, 1000)\n",
    "except:\n",
    "    print(\"Could not find initial state\")\n",
    "\n",
    "# Initialize kalman filters\n",
    "try:\n",
    "    kf_pos = kalman.Kalman(_acc_variance=0.5, _type=\"pose\", init_x=np.hstack((pos,[0,0])))\n",
    "    kf_att = kalman.Kalman(_acc_variance=0.5, _type=\"orientation\", init_x=np.hstack((orientation,0)))\n",
    "except:\n",
    "    print(\"Could not initialize Kalman filters\")\n",
    "\n",
    "print(\"Initialization complete: %5.3f s\" % (time.time()-t))\n",
    "print(\"Initial pose:\", kf_pos.mean)\n",
    "print(\"Initial orientation\", kf_att.mean*180/np.pi)\n",
    "print(\"--------------------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation Global Navigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% Run thymio\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start global navigation initialization\n",
      "goal found: 0.006 s\n",
      "Initialization complete: 0.345 s\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Global navigation\n",
    "print(\"Start global navigation initialization\")\n",
    "t = time.time()\n",
    "goal = vision.goalFetch(vid)\n",
    "print(\"goal found: %5.3f s\" % (time.time()-t))\n",
    "nodes, nodeCon, maskObsDilated, optimal_path = gn.opt_path(vid, goal)\n",
    "\n",
    "print(\"Initialization complete: %5.3f s\" % (time.time()-t))\n",
    "print(\"--------------------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation Motion Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%% Run thymio\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start motion control initialization\n",
      "Initialization complete: 0.001 s\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Motion control\n",
    "t = time.time()\n",
    "print(\"Start motion control initialization\")\n",
    "\n",
    "mc = motion_control.MotionControl(optimal_path*100,\n",
    "                                  kf_pos.mean[0]*100,\n",
    "                                  kf_pos.mean[1]*100,\n",
    "                                  kf_att.mean[0],\n",
    "                                  nodes,\n",
    "                                  nodeCon,\n",
    "                                  maskObsDilated,\n",
    "                                  goal)\n",
    "\n",
    "print(\"Initialization complete: %5.3f s\" % (time.time()-t))\n",
    "print(\"--------------------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%% Run thymio\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "No vision measurements found\n",
      "65.85563506130639 39.56082017993816\n",
      "67.4537268694592 40.91792921772191\n",
      "65.85563506130639 39.56082017993816\n",
      "67.45429419089326 40.946562380404266\n",
      "65.85563506130639 39.56082017993816\n",
      "67.45462701478844 40.96194971345502\n",
      "65.85563506130639 39.56082017993816\n",
      "67.45495773495126 40.9769556477863\n",
      "65.85563506130639 39.56082017993816\n",
      "67.45521491906892 40.98820408496397\n",
      "65.85563506130639 39.56082017993816\n",
      "67.45719670799504 41.04934024833875\n",
      "65.85563506130639 39.56082017993816\n",
      "67.45889608055234 41.094558767940825\n",
      "65.85563506130639 39.56082017993816\n",
      "67.46038616110997 41.17269507551415\n",
      "65.85563506130639 39.56082017993816\n",
      "67.50435647491308 41.280508272634975\n",
      "65.85563506130639 39.56082017993816\n",
      "67.5505757668867 41.448390239742395\n",
      "65.85563506130639 39.56082017993816\n",
      "67.58505288883441 41.618112264976155\n",
      "65.85563506130639 39.56082017993816\n",
      "67.6119437436747 41.844155268191756\n",
      "65.85563506130639 39.56082017993816\n",
      "67.6315885631408 42.062004177450994\n",
      "65.85563506130639 39.56082017993816\n",
      "67.69818142167439 42.259385045720634\n",
      "65.85563506130639 39.56082017993816\n",
      "67.73970767713557 42.40517956310582\n",
      "65.85563506130639 39.56082017993816\n",
      "67.77148202548659 42.55823346417856\n",
      "65.85563506130639 39.56082017993816\n",
      "67.79657552128452 42.7607031792655\n",
      "65.85563506130639 39.56082017993816\n",
      "67.87109232287126 43.02252231242635\n",
      "65.85563506130639 39.56082017993816\n",
      "67.91953144409244 43.221320494390234\n",
      "65.85563506130639 39.56082017993816\n",
      "67.95557679482158 43.411298545670654\n",
      "65.85563506130639 39.56082017993816\n",
      "68.0222693484065 43.587716776495085\n",
      "65.85563506130639 39.56082017993816\n",
      "68.09127387096339 43.8254550052537\n",
      "65.85563506130639 39.56082017993816\n",
      "68.1313402194279 43.95688114834257\n",
      "65.85563506130639 39.56082017993816\n",
      "68.15941823908793 44.13470298492504\n",
      "65.85563506130639 39.56082017993816\n",
      "68.18054935840192 44.360058123169864\n",
      "65.85563506130639 39.56082017993816\n",
      "68.20206665726182 44.57838468728547\n",
      "65.85563506130639 39.56082017993816\n",
      "68.25407622975963 44.742079648644115\n",
      "65.85563506130639 39.56082017993816\n",
      "68.29280851327438 44.90232674300245\n",
      "65.85563506130639 39.56082017993816\n",
      "68.32211430851723 45.065106416718024\n",
      "65.85563506130639 39.56082017993816\n",
      "68.34425223138423 45.23173356985038\n",
      "65.85563506130639 39.56082017993816\n",
      "68.4010752321192 45.43743558849591\n",
      "65.85563506130639 39.56082017993816\n",
      "68.44458588340073 45.590993589069136\n",
      "65.85563506130639 39.56082017993816\n",
      "68.479653543912 45.75343532393629\n",
      "65.85563506130639 39.56082017993816\n",
      "68.50630149073076 45.9177909493014\n",
      "65.85563506130639 39.56082017993816\n",
      "68.52661392417735 46.087651778267045\n",
      "65.85563506130639 39.56082017993816\n",
      "68.54079289451673 46.257973496405484\n",
      "65.85563506130639 39.56082017993816\n",
      "68.59141772548587 46.42950944964122\n",
      "65.85563506130639 39.56082017993816\n",
      "68.63412872553934 46.6116512630578\n",
      "65.85563506130639 39.56082017993816\n",
      "68.66608331145517 46.78620041795442\n",
      "65.85563506130639 39.56082017993816\n",
      "68.69036191406066 47.00317751685219\n",
      "65.85563506130639 39.56082017993816\n",
      "68.70922000388879 47.222104826170224\n",
      "65.85563506130639 39.56082017993816\n",
      "68.76109840565428 47.41475833477107\n",
      "65.85563506130639 39.56082017993816\n",
      "68.80004054813654 47.592630626122975\n",
      "65.85563506130639 39.56082017993816\n",
      "68.8281147327288 47.75645014451263\n",
      "65.85563506130639 39.56082017993816\n",
      "68.85564955162322 47.95253522628687\n",
      "65.85563506130639 39.56082017993816\n",
      "68.91543737481734 48.143590791550636\n",
      "65.85563506130639 39.56082017993816\n",
      "68.96301972678862 48.327073172897826\n",
      "65.85563506130639 39.56082017993816\n",
      "69.00025020765753 48.50689304841546\n",
      "65.85563506130639 39.56082017993816\n",
      "69.02963968985725 48.688361396462426\n",
      "65.85563506130639 39.56082017993816\n",
      "69.05382031314335 48.880069162152736\n",
      "65.85563506130639 39.56082017993816\n",
      "69.11045900891908 49.06225340038329\n",
      "65.85563506130639 39.56082017993816\n",
      "69.15553360480331 49.24141718067409\n",
      "65.85563506130639 39.56082017993816\n",
      "69.19132430821547 49.422771805557346\n",
      "65.85563506130639 39.56082017993816\n",
      "69.21583345982117 49.59047047123421\n",
      "65.85563506130639 39.56082017993816\n",
      "69.27619752734641 49.76759371592961\n",
      "65.85563506130639 39.56082017993816\n",
      "69.32453927644137 49.944542537932875\n",
      "65.85563506130639 39.56082017993816\n",
      "69.36241335149988 50.12306920475101\n",
      "65.85563506130639 39.56082017993816\n",
      "69.39107497981706 50.30243487194134\n",
      "65.85563506130639 39.56082017993816\n",
      "69.41006666535448 50.47210666008428\n",
      "65.85563506130639 39.56082017993816\n",
      "69.42317190966307 50.640632360780344\n",
      "65.85563506130639 39.56082017993816\n",
      "69.47467698926509 50.82201679486846\n",
      "65.85563506130639 39.56082017993816\n",
      "69.51653668723321 51.00591504902434\n",
      "65.85563506130639 39.56082017993816\n",
      "69.54517352363607 51.16988581262111\n",
      "65.85563506130639 39.56082017993816\n",
      "69.56762335840322 51.381864304650804\n",
      "65.85563506130639 39.56082017993816\n",
      "69.62566890567005 51.59095899243859\n",
      "65.85563506130639 39.56082017993816\n",
      "69.6706031558192 51.746413617536135\n",
      "65.85563506130639 39.56082017993816\n",
      "69.70685640581881 51.95072614813078\n",
      "65.85563506130639 39.56082017993816\n",
      "69.7337065428131 52.10558846568743\n",
      "65.85563506130639 39.56082017993816\n",
      "69.7976785142257 52.319059836247895\n",
      "65.85563506130639 39.56082017993816\n",
      "69.84827385114725 52.523673985575925\n",
      "65.85563506130639 39.56082017993816\n",
      "69.88409139513415 52.7084205042235\n",
      "65.85563506130639 39.56082017993816\n",
      "69.91015608285561 52.88637133830167\n",
      "65.85563506130639 39.56082017993816\n",
      "69.93032394315665 53.06855031795258\n",
      "65.85563506130639 39.56082017993816\n",
      "69.97944349924498 53.22416580015988\n",
      "65.85563506130639 39.56082017993816\n",
      "70.02430490230009 53.408864243998124\n",
      "65.85563506130639 39.56082017993816\n",
      "70.0599590856663 53.593302209309876\n",
      "65.85563506130639 39.56082017993816\n",
      "70.08504982906145 53.76500244386369\n",
      "65.85563506130639 39.56082017993816\n",
      "70.10560018877223 53.985683841310774\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jacqu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:3441: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Initialize pygame\n",
    "pg.init()\n",
    "\n",
    "h = int(841 / 2)\n",
    "w = int(1189 / 2)\n",
    "scr = pg.display.set_mode((w, h))\n",
    "\n",
    "thymio = pg.image.load(\"img/thymio.png\")\n",
    "thymio = pg.transform.scale(thymio, (50, 50))\n",
    "\n",
    "# main loop\n",
    "running = True\n",
    "simulate = False\n",
    "local = False\n",
    "while running:\n",
    "    # Configure key presses for pygame and cv2 windows\n",
    "    for event in pg.event.get():\n",
    "        if event.type == pg.QUIT:\n",
    "            cv2.destroyAllWindows()\n",
    "            pg.quit()\n",
    "            sys.exit()\n",
    "        elif event.type == pg.KEYDOWN:\n",
    "            if event.key == pg.K_RETURN :\n",
    "                simulate = not simulate\n",
    "    # Set up pygame window\n",
    "    scr.fill((0, 0, 0))\n",
    "    pg.draw.line(scr, (255, 255, 255), (0, int(h/2)), (w, int(h/2)))\n",
    "    pg.draw.line(scr, (255, 255, 255), (int(w/2), 0), (int(w/2), h))\n",
    "\n",
    "    # Start thymio movement\n",
    "    if simulate:\n",
    "        timer = time.time()\n",
    "        # Save expected values and covariance matrices\n",
    "        mus_pos.append(kf_pos.mean)\n",
    "        mus_att.append(kf_att.mean)\n",
    "\n",
    "        # Read sensor data\n",
    "        try:\n",
    "            pos, orientation = vision.fetchPoseMeters(vid)\n",
    "            fetched = True\n",
    "        except:\n",
    "            fetched = False\n",
    "            print(\"No vision measurements found\")\n",
    "\n",
    "        motor_speeds = read_motors()\n",
    "        speed = (sum(motor_speeds)/2)*0.0004                                # magnitude of thymio's linear velocity [m/s]\n",
    "        angular_speed = (motor_speeds[1]-motor_speeds[0])*0.0004/0.094      # magnitude of thymio's angular velocity [rad/s]\n",
    "        vel = [speed*np.cos(mus_att[-1][0]), speed*np.sin(mus_att[-1][0])]\n",
    "\n",
    "        # Estimate pose\n",
    "        kalman.estimate_pose(time.time()-t, kf_pos, vel, pos, fetched)\n",
    "\n",
    "        # Estimate orientation\n",
    "        kalman.estimate_orientation(time.time()-t, kf_att, angular_speed, orientation, fetched)\n",
    "\n",
    "        # Motion control\n",
    "        prox_front = read_proximity()\n",
    "        mc.update_motion(kf_pos.mean[0]*100, kf_pos.mean[1]*100, kf_att.mean[0], prox_front, vid)\n",
    "        \n",
    "        # Update motor speeds\n",
    "        motors(l_speed=int(mc.l_speed), r_speed=int(mc.r_speed))\n",
    "\n",
    "        # Draw Thymio and optimal path in window\n",
    "        kalman.move(kf_pos.mean[0], kf_pos.mean[1], kf_att.mean[0], w, h, thymio, scr)\n",
    "        kalman.drawLine([mu[0] for mu in mus_pos], [mu[1] for mu in mus_pos], w, h, scr)\n",
    "        kalman.path(mc.optimal_path/100, w, h, scr)\n",
    "    else:\n",
    "        kalman.move(kf_pos.mean[0], kf_pos.mean[1], kf_att.mean[0], w, h, thymio, scr)\n",
    "        kalman.drawLine([mu[0] for mu in mus_pos], [mu[1] for mu in mus_pos], w, h, scr)\n",
    "        kalman.path(mc.optimal_path/100, w, h, scr)\n",
    "        motors(0, 0)\n",
    "\n",
    "    # Update pygame and cv2 windows\n",
    "    optimal_path = mc.optimal_path/100\n",
    "    try:\n",
    "        output = vision.liveFeedback(vid, mc.nodes, mc.nodeCon, mc.maskObsDilated, optimal_path)\n",
    "    except:\n",
    "        pass\n",
    "    else:\n",
    "        cv2.imshow('Live Feedback', output)\n",
    "\n",
    "    pg.display.flip()\n",
    "    t = time.time()\n",
    "\n",
    "print(\"===================================\")\n",
    "print(\"-------------Finished--------------\")\n",
    "print(\"===================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "953b094420f6655e8472f1b5a39dd9b0518e85b962014011ca5adb24391c5919"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73096b5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2061228152.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [2], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    ***\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Hot-Wheels - Thymio Path Finder Project (Mobile Robotics) \n",
    "##### _The best vision based path findind program for Thymio_\n",
    "***\n",
    "\n",
    "## Abstract\n",
    "<div style=\"text-align: justify\"> This project is part of the course Basics of mobile robotics (ME-452). The goal is to create a software enabling the Thymio to find its path through a set of obstacles. To this end, it will need to localize itself as well as its environment, and find the best path to the goal to then drive to it. Another challenge is for it to avoid any unexpected object it meets. This project is thus split in four distinct modules, plus the main file. Those four modules are: </div>\n",
    "\n",
    "- <b>Vision</b>, camera based function and transformation for position, goal and terrain exctraction\n",
    "- <b>Kalman Filter</b>, sensor fusion for accurate odometry computation\n",
    "- <b>Optimal path algorithm</b>, path optimization using Dijkstra\n",
    "- <b>Motion control</b>, in charge of the Thymio's motion and local avoidance\n",
    "\n",
    "Those modules are further detailed bellow:\n",
    "\n",
    "## Table of contents\n",
    "1. [Vision](#1-vision)\n",
    "2. [Kalman Filter](#2-kalman-filter)\n",
    "    - [Position Kalman Filter](#21-position-kalman-filter)\n",
    "    - [Orientation kalman filter](#22-orientation-kalman-filter)\n",
    "    - [Kalman filter update squence](#23-kalman-filter-update-sequence)\n",
    "3. [Optimal path algorithm](#3-optimal-path-algorithm)\n",
    "4. [Motion control](#4-motion-control)\n",
    "    - [Thymio's bivalent behavior](#41-thymios-bivalent-behavior)\n",
    "    - [Global path following](#42-global-path-following)\n",
    "    - [Local avoidance](#local-avoidance)\n",
    "5. [Demo](#5-demo)\n",
    "\n",
    "## Vision\n",
    "\n",
    "This module includes functions converting the information percieved by the camera into a map, a goal and the current position of the robot. To this end, the image obtained from the camera is first filtered using a median filter, useful to remove the noise while preserving clear edges.\n",
    "\n",
    "The obstacles are represented by black shapes. In order to fetch the map, function <b>terrainFetch</b> computing the visibility graph is called. Pixel segmentation is done on the retrieved image, followed by a blob analysis only conserving blobs of a certain size (filtering leftover noise). These blobs are then dilated to account for the robot's size, and the corners for each dilated blob are computed. The start and end point (i.e. start position of the robot and goal position) are also fetched (using functions bellow), and added to the previous nodes (i.e. corners of the dilated blobs). To check weither the nodes are connected, each node is iterating over all the node, checking wether it has a direct connection (without intersecting a dilated blobs). This function returns the position of the nodes in meters, their connections, and the dilated obstacles' mask (displayed in the visual interface).\n",
    "\n",
    "Function <b>poseFetch</b> fetches the robots' position and angle thanks to the Thymio's red (angle) and green (position & angle) LEDs. The position is computed by first running a pixel segmentation and blob analysis to only keep the two green blobs. The center of those two blobs is found and the middle between those two points is defined to be the position of the robot. The center of the red blob is obtained with the same transformations (this time for one blob only), and the angle of the robot is resolved when comparing the two center found (center of the red blob, and center of the two green blobs) with respect to the X axis (i.e. using the arctan). The position of the robot is returned in pixel since this function is used in <b>terrainFetch</b>, but the position in meter is returned by the function <b>fetchOdoMeters</b>, calling <b>odoFetch</b> multiple times and converting the result in meter.\n",
    "\n",
    "The goal represented by a red square piece of paper is fetched with function <b>goalFetch</b>. It is retreived by performing the same transformation as done for the position : pixel segmentation followed by blob analysis to check the size of the blob and avoid noise. The goal is thus the center of the blob and is returned in meters.\n",
    "\n",
    "In order to get a visual feedback on what the algorithm outputs, function <b>liveFeedback</b> was created, it prints each node and their connections, the dilated obstacles, the robot's current position and angle, and the goal position.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"img/ThymioRun.png\" width=\"564\" height=\"400\">\n",
    "</p>\n",
    "\n",
    "## Kalman Filter\n",
    "\n",
    "The state of the Thymio is estimated using a Kalman filter. The choice for a kalman filter was made because we decided for the global navigation to use nodes described in a coordinate system with x and y coordinates. Thus, also the position of the Thymio should be estimated in that coordinate system. Since we have multiple sensor readings: vision and wheel encoder, the Kalman filter was selected to fuse these measurements.\n",
    "\n",
    "Since the position dynamics are independent of the orientation dynamics, two Kalman filters are used. Both are initialized using a vision measurement.\n",
    "\n",
    "### Position Kalman Filter\n",
    "The dynamic system used for the position estimation:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_{k+1} \\\\ y_{k+1} \\\\ v_{x_{k+1}} \\\\ v_{y_{k+1}}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & T & 0 \\\\\n",
    "0 & 1 & 0 & T \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{k} \\\\ y_{k} \\\\ v_{x_{k}} \\\\ v_{y_{k}}\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "\\frac{T^2}{2} & 0 \\\\\n",
    "0 & \\frac{T^2}{2} \\\\\n",
    "T & 0 \\\\\n",
    "0 & T \\\\\n",
    "\\end{bmatrix}\\boldsymbol{w}_k\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_{1_k} = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0\\\\\n",
    "0 & 1 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{k} \\\\ y_{k} \\\\ v_{x_{k}} \\\\ v_{y_{k}}\n",
    "\\end{bmatrix} + \\boldsymbol{v}_{k_1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_{2_k} = \\begin{bmatrix}\n",
    "0 & 0 & 1 & 0\\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_{k} \\\\ y_{k} \\\\ v_{x_{k}} \\\\ v_{y_{k}}\n",
    "\\end{bmatrix} + \\boldsymbol{v}_{k_2}\n",
    "$$\n",
    "\n",
    "The $x$ and $y$ position coordinates are in meters and the $v_x$ and $v_y$ velocities are in m/s. The output measurement $y_{1_k}$ consists of the position and represents the vision measurements. Output measurement $y_{2_k}$ consists of the velocity and represents the wheel encoder measurements from which the Thymio's linear velocity is determined. Following covariance matrices have been defines:\n",
    "\n",
    "$$\n",
    "W = \n",
    "\\begin{bmatrix}\n",
    "\\frac{T^2}{2} & 0 \\\\\n",
    "0 & \\frac{T^2}{2} \\\\\n",
    "T & 0 \\\\\n",
    "0 & T \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\sigma_{acc}^2 & 0 \\\\ \n",
    "0 & \\sigma_{acc}^2 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{T^2}{2} & 0 & T & 0 \\\\\n",
    "0 & \\frac{T^2}{2} & 0 & T\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "V_1 = \n",
    "\\begin{bmatrix}\n",
    "0.0001 & 0 \\\\\n",
    "0 & 0.0001 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "V_2 = \n",
    "\\begin{bmatrix}\n",
    "0.01 & 0 \\\\\n",
    "0 & 0.01 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The variance matrix $W$ corresponds to the process noise term $\\boldsymbol{w}_k$ and the variance matrices $V_1$ and $V_2$ correspond to the output noise terms $\\boldsymbol{v}_{k_1}$ and $\\boldsymbol{v}_{k_2}$. The parameter $\\sigma_{acc}$ represents the exptected standard deviation in linear acceleration. Since a constant velocity model is assumed, it accounts for model mismatch. $T$ is the time passed since the last estimate has been made.\n",
    "\n",
    "The output covariance matrix $V_1$ was chosen to have 0.0001 on its diagonal since this corresponds to a standard deviation of 0.01m or an uncertainty of 1cm in the position measurement. The output covariance matrix $V_2$ was chose to have 0.01 on its diagonal since this corresponds to a standard deviation of 0.1m/s or an uncertainty of 10cm/s in the angular velocity measurement.\n",
    "\n",
    "\n",
    "### Orientation Kalman Filter\n",
    "The dynamic system used for the orientation estimation:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\theta_{k+1} \\\\ \\omega_{k+1}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "1 & T \\\\\n",
    "0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\theta_{k} \\\\ \\omega_{k}\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "\\frac{T^2}{2} \\\\\n",
    "T\n",
    "\\end{bmatrix}\\boldsymbol{w}_k\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_{1_k} = \\begin{bmatrix}\n",
    "1 & 0 \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\theta_{k} \\\\ \\omega_{k}\n",
    "\\end{bmatrix} + \\boldsymbol{v}_{k_1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_{2_k} = \\begin{bmatrix}\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\theta_{k} \\\\ \\omega_{k}\n",
    "\\end{bmatrix} + \\boldsymbol{v}_{k_2}\n",
    "$$\n",
    "\n",
    "The orientation angle $\\theta$ is in radians and the angular velocity $\\omega$ is in rad/s. The output measurement $y_{1_k}$ consists of the orientation and represents the vision measurements. Output measurement $y_{2_k}$ consists of the angular velocity and represents the wheel encoder measurements from which the Thymio's angular velocity is determined. Following covariance matrices have been defines:\n",
    "$$\n",
    "W = \n",
    "\\begin{bmatrix}\n",
    "\\frac{T^2}{2} \\\\\n",
    " T \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\sigma_{acc}^2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{T^2}{2} & T\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "V_1 = \n",
    "\\begin{bmatrix}\n",
    "0.0001\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "V_2 = \n",
    "\\begin{bmatrix}\n",
    "0.01\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "The variance matrix $W$ corresponds to the process noise term $\\boldsymbol{w}_k$ and the variance matrices $V_1$ and $V_2$ correspond to the output noise terms $\\boldsymbol{v}_{k_1}$ and $\\boldsymbol{v}_{k_2}$. The parameter $\\sigma_{acc}$ represents the exptected standard deviation in angular acceleration. Since a constant angular velocity model is assumed, it accounts for model mismatch. $T$ is the time passed since the last estimate has been made.\n",
    "\n",
    "The output covariance matrix $V_1$ was chosen to have 0.0001 since this corresponds to a standard deviation of 0.01rad or an uncertainty of 0.57deg in the orientation measurement. The output covariance matrix $V_2$ was chose to have 0.01 since this corresponds to a standard deviation of 0.1rad/s or an uncertainty of 5.7deg/s in the velocity measurement.\n",
    "\n",
    "### Kalman filter update sequence\n",
    "The estimate of Thymio's state is performed through the following sequence of calculations:\n",
    "1. prediction step\n",
    "- $\\hat{x}_{k}^- = A\\hat{x}_{k-1}^+$\n",
    "- $P_k^- = AP_{k-1}^+A^T + W$\n",
    "\n",
    "2. update step vision measurement (if available)\n",
    "- $K_k = P_k^-C_{1}^T(C_1P_k^-C_1^T + V_1)^{-1}$\n",
    "- $\\hat{x}_k^+ = \\hat{x}_k^- + K_k(y_{1_k}-C_{1}\\hat{x}_k^-)$\n",
    "- $P_k^+ = (I-K_kC_1)P_k^-$\n",
    "\n",
    "3. update step wheel encoder (if available)\n",
    "- $K_k = P_k^-C_{2}^T(C_1P_k^-C_2^T + V_2)^{-1}$\n",
    "- $\\hat{x}_k^+ = \\hat{x}_k^- + K_k(y_{2_k}-C_{2}\\hat{x}_k^-)$\n",
    "- $P_k^+ = (I-K_kC_2)P_k^-$\n",
    "\n",
    "There three step are executed at each timestep and are the same for the position as well as the orientation kalman filter.\n",
    "\n",
    "## Optimal Path Algorithm\n",
    "<div style=\"text-align: justify\">\n",
    "To find the optimal path for the robot, we decide to implement Dijkstra’s algorithm on our project. The vision module extract all coordinates of nodes and edges from obstacles.  This data is sent thanks to the function <b>terrainFetch</b>. We run this function until detect the map correctly. This step is done on the function <b>opt_path </b>. When it’s done, we run the Dijkstra algorithm with our function <b>dijkstra </b>. We have created node and edges classes in order to have a better readability.\n",
    "<br/><br/>\n",
    "In this function, we initialize a list of nodes and another one with edges. Note that we have to duplicate edges to have both ways, because the vision module gives us only one way. Then, we define two lists to save the length of the path, and its coordinates of each node and one dictionary, which allows us to know the index of each node on its lists. \n",
    "<br/> <br/>\n",
    "We initialise three variables to define the current situation (current node, distance between it and the starting point, all nodes of its path).  The <b>iteration </b> variable helps us to define the future node after each iteration.\n",
    "<br/> <br/>\n",
    "As long as the edges list isn’t empty, we calculate the distance between the start and end point of each edge having as starting point the current node. Then, we update the two array <b>tabLenpath </b> and <b>tabPath </b>. I remove all used edges and ones which, the end point is the current node. At the end, we updated the situation increasing iteration by one and updating the three variables. We look for the i-th smallest value in the array <b>ablenPath</b>. <b>idx_min </b>takes the index of this value and then the corresponding node (found using <b>Nodes</b> and <b>idx_min</b>) is assigned to <b>act_node</b>. The <b> actu_path </b> list is updated using <b>idx_min</b> and <b>tabPath</b>. Iteration will then just go to the number of nodes. We have limited the iterations by putting a condition in the while loop, to avoid having an infinite loop, in case the goal is isolated.\n",
    "<br/> <br/>\n",
    "When edge list is empty, we extract the optimal path by tabPath and index of goal position. In order for the motion control module to be able to use the output correctly we transform the list into an array.\n",
    "</div>\n",
    "\n",
    "## Motion Control\n",
    "\n",
    "### Thymio's bivalent behavior\n",
    "<div style=\"text-align: justify\">The motion control part of the program takes care of both the global path following and the local object avoidance. Follows the finite state machine for the controller to go from the global to the local behavior. Each time the controller quits the local loop, a new optimal path is recomputed to avoid collisions with walls on the map.\n",
    "\n",
    "![FSM Global to local](./img/FSM-GlobalToLocal.png \"Global to local behaviour FSM\")\n",
    "\n",
    "This FSM is implemented in the function `update_motion()` in the file [**motion_control.py**](motion_control.py) and is in charge of choosing which behavior to adopt to compute the update the wheels speed. Three functions are called :\n",
    " - `update_global()` to compute the wheels speed to follow the global optiml path\n",
    " - `update_local_pivot()` to pivot left when an obstacle is detected\n",
    " - `update_local_fwd()` to move forward 15cm once the obstacle is not in the path anymore\n",
    "\n",
    "### Global path following\n",
    "Because Thymio is a two-wheeled robot it can go strait forward by setting the right and left wheelspeed to the same value, and it can pivot by setting the wheel speed to opposite values. For simplicity and robustness to path-following, these are the two basic control movements we used to control our Thymio's global motion.\n",
    "\n",
    "We have decided to use a proportional controller for both the forward and pivot motion: the further the robot's state from the desired one, the faster the movement.</div>\n",
    "| Motion | Inputs | Computes | Outputs |\n",
    "|---:|---|---|---|\n",
    "| **Forward** | - Position of the robot, target point | - Distance to the target point | Wheel speed = $K \\cdot d(robot,target) + C$|\n",
    "| **Pivot** | - Angle of the robot, target point | - Difference in angle between the robot and the target point | Wheel speed = $\\pm K \\cdot \\alpha(robot,target) + C$|\n",
    "\n",
    "\n",
    "> It is important to note that the right and left wheel speeds differ a bit according the the angle between Thymio and the target point during the forward motion in order to track better the target. For example if the angle of Thymio is a bit off by $-\\pi/8$ then the right wheel will be a bit faster than the left wheel.\n",
    "\n",
    "> The proportional terms K and the offset values C were set after some tries on hardware in order to track the target point at best without having the robot going too fast.\n",
    "\n",
    "<div style=\"text-align: justify\">\n",
    "The below diagram explains what is the logic behind the optimal path following controller.\n",
    "\n",
    "![Global path following](./img/GlobalPathFollowing.png \"Optimal path following\")\n",
    "\n",
    "### Local avoidance\n",
    "In case there is an non intended object on the pre-computed optimal path, Thymio cannot pass through and has to find a way to get around it.\n",
    "\n",
    "We've tried first to implement a neuronal network controller as in class, each sensor having a weight on the robot's wheels speed. However Thymio only has front sensors and couldn't detect if an object is on its side, which often resulted in the robot to blindly charge into the obstacle:\n",
    "\n",
    "![Neuronal network problem](./img/NeuronalNetwork.png \"Blindly charge the obstacle\")\n",
    "\n",
    "The other solution was to pivot the robot 90° left once he detect something on the sensors, then do a right arc until either the obstacle is detected again or if the robot faces the target point again.\n",
    "\n",
    "This solution wasn't successful as the robot's arc radius is highly dependant on the object's size and the robot often ended charging the obstacle as in the previous method.\n",
    "\n",
    "Finally we decided to implement this final method. It's simple to implement and more robust than anything we tried before.\n",
    "\n",
    "```\n",
    "while sensors:\n",
    "    pivot left\n",
    "while no sensors:\n",
    "    go forward for 15 cm\n",
    "recompute optimal path\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "## Demo\n",
    "\n",
    "<div style=\"text-align: justify\">\n",
    "This last part shows a simulation with three different views: the first one is an external camera shot, the second one is the real time feedback seen by the camera with the implementation of the vision and the optimal path and the last one is a plot used to observe the approximation of the robot position and its angle using the Kalmann filter\n",
    "<div>\n",
    "|                                    |                          |\n",
    "| ---------------------------------- | ------------------------ |\n",
    "| Description                        |                          |\n",
    "| Simulation from external camera    | ![](camera.gif)    |\n",
    "| Image Processing from Webcam       | ![](feedback.gif)  |\n",
    "| Kalman filter display              | ![](pygame.gif)    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1137c813",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
